No augmentation or normalization
BS = 32
INIT_LR = 1e-2
NUM_EPOCHS = 25

VGGMini without last convolution layer
              precision    recall  f1-score   support

    accuracy                           0.93     10000
   macro avg       0.94      0.93      0.94     10000
weighted avg       0.94      0.93      0.94     10000


VGGMini
              precision    recall  f1-score   support

    accuracy                           0.94     10000
   macro avg       0.94      0.94      0.94     10000
weighted avg       0.94      0.94      0.94     10000


VGGMiniCBR
              precision    recall  f1-score   support

    accuracy                           0.94     10000
   macro avg       0.94      0.94      0.94     10000
weighted avg       0.94      0.94      0.94     10000


10degrees
              precision    recall  f1-score   support

    accuracy                           0.94     10000
   macro avg       0.94      0.94      0.94     10000
weighted avg       0.94      0.94      0.94     10000


0.05translate
              precision    recall  f1-score   support

    accuracy                           0.95     10000
   macro avg       0.95      0.95      0.95     10000
weighted avg       0.95      0.95      0.95     10000


hflip
              precision    recall  f1-score   support

    accuracy                           0.94     10000
   macro avg       0.94      0.94      0.94     10000
weighted avg       0.94      0.94      0.94     10000


NormalizationPerDataset mean: 0.2860  std: 0.3530
              precision    recall  f1-score   support

    accuracy                           0.94     10000
   macro avg       0.94      0.94      0.94     10000
weighted avg       0.94      0.94      0.94     10000


NormMinus1Only -1, 1
              precision    recall  f1-score   support

    accuracy                           0.94     10000
   macro avg       0.94      0.94      0.94     10000
weighted avg       0.94      0.94      0.94     10000


Conclusion: Normalization doesnot matter.

0.1translate
evaluating network...
              precision    recall  f1-score   support

    accuracy                           0.94     10000
   macro avg       0.94      0.94      0.94     10000
weighted avg       0.94      0.94      0.94     10000

Conclusion: 0.05 translation is better.


degrees=20, translate=(0.1, 0.1), scale=(0.9, 1.1),shear=0.5 with flip
              precision    recall  f1-score   support

    accuracy                           0.92     10000
   macro avg       0.92      0.92      0.92     10000
weighted avg       0.92      0.92      0.92     10000


degrees=10, translate=(0.05, 0.05), scale=(0.9, 1.1),shear=10 without flip
              precision    recall  f1-score   support

    accuracy                           0.94     10000
   macro avg       0.94      0.94      0.94     10000
weighted avg       0.94      0.94      0.94     10000


moderate degrees=5, translate=(0.03, 0.03), scale=(0.95, 1.05),shear=5 without flip << much better and with flipping its till 94
              precision    recall  f1-score   support

    accuracy                           0.95     10000
   macro avg       0.95      0.95      0.95     10000
weighted avg       0.95      0.95      0.95     10000


moderate with zca << not performing good, stick with moderate.
              precision    recall  f1-score   support

    accuracy                           0.94     10000
   macro avg       0.94      0.94      0.94     10000
weighted avg       0.94      0.94      0.94     10000


Batch256 moderate_256
             precision    recall  f1-score   support

    accuracy                           0.94     10000
   macro avg       0.94      0.94      0.94     10000
weighted avg       0.94      0.94      0.94     10000

Batch256 moderate_256_adam <-- should be used
              precision    recall  f1-score   support

    accuracy                           0.95     10000
   macro avg       0.95      0.95      0.95     10000
weighted avg       0.95      0.95      0.95     10000
